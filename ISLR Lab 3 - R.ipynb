{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Logistic Regression, LDA, QDA, and KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will go over how to perform logistic regression, linear and quadratic discriminant analysis, and k-nearest neighbors in order to predict categorical responses in `R`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Stock Market Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll look at some numerical and graphical summaries of the `Smarket` data set from the `ISLR` library. It consists of the percentage returns for the S&P 500 stock index over 1,250 days, from the beginning of 2001 until the end of 2005. For each date, there are the following pieces of information:\n",
    "\n",
    "- `Lag1` through `Lag5`: the percentage returns for each of the five previous trading days\n",
    "- `Volume`: the number of shares traded on the previous day, in billions\n",
    "- `Today`: the percentage return on the date in question\n",
    "- `Direction`: whether the market was `Up` (positive percentage return) or `Down` (negative percentage return) on this date.\n",
    "\n",
    "Let's load the ISLR library and then also export the `Smarket` set to a CSV file for future use when I go back to do this lab in Python. After that, we'll take a peak at a basic summary of the set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ISLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "write.csv(Smarket, \"Smarket.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Year'</li>\n",
       "\t<li>'Lag1'</li>\n",
       "\t<li>'Lag2'</li>\n",
       "\t<li>'Lag3'</li>\n",
       "\t<li>'Lag4'</li>\n",
       "\t<li>'Lag5'</li>\n",
       "\t<li>'Volume'</li>\n",
       "\t<li>'Today'</li>\n",
       "\t<li>'Direction'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Year'\n",
       "\\item 'Lag1'\n",
       "\\item 'Lag2'\n",
       "\\item 'Lag3'\n",
       "\\item 'Lag4'\n",
       "\\item 'Lag5'\n",
       "\\item 'Volume'\n",
       "\\item 'Today'\n",
       "\\item 'Direction'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Year'\n",
       "2. 'Lag1'\n",
       "3. 'Lag2'\n",
       "4. 'Lag3'\n",
       "5. 'Lag4'\n",
       "6. 'Lag5'\n",
       "7. 'Volume'\n",
       "8. 'Today'\n",
       "9. 'Direction'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n",
       "[7] \"Volume\"    \"Today\"     \"Direction\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names(Smarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1250</li>\n",
       "\t<li>9</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1250\n",
       "\\item 9\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1250\n",
       "2. 9\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1250    9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(Smarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Year           Lag1                Lag2                Lag3          \n",
       " Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n",
       " 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n",
       " Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n",
       " Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n",
       " 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n",
       " Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n",
       "      Lag4                Lag5              Volume           Today          \n",
       " Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n",
       " 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n",
       " Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n",
       " Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n",
       " 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n",
       " Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n",
       " Direction \n",
       " Down:602  \n",
       " Up  :648  \n",
       "           \n",
       "           \n",
       "           \n",
       "           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(Smarket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, before moving forward, we'll produce a scatterplot matrix of all the variables in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAABlBMVEUAAAD///+l2Z/dAAAA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\ngg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pairs(Smarket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's produce a matrix of all the pairwise correlations among the variables in the data set. Note that since `Direction` is a qualitative variable we need to exclude it when using the `cor()` function. Otherwise, we'll get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Year</th><th scope=col>Lag1</th><th scope=col>Lag2</th><th scope=col>Lag3</th><th scope=col>Lag4</th><th scope=col>Lag5</th><th scope=col>Volume</th><th scope=col>Today</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Year</th><td>1.00000000  </td><td> 0.029699649</td><td> 0.030596422</td><td> 0.033194581</td><td> 0.035688718</td><td> 0.029787995</td><td> 0.53900647 </td><td> 0.030095229</td></tr>\n",
       "\t<tr><th scope=row>Lag1</th><td>0.02969965  </td><td> 1.000000000</td><td>-0.026294328</td><td>-0.010803402</td><td>-0.002985911</td><td>-0.005674606</td><td> 0.04090991 </td><td>-0.026155045</td></tr>\n",
       "\t<tr><th scope=row>Lag2</th><td>0.03059642  </td><td>-0.026294328</td><td> 1.000000000</td><td>-0.025896670</td><td>-0.010853533</td><td>-0.003557949</td><td>-0.04338321 </td><td>-0.010250033</td></tr>\n",
       "\t<tr><th scope=row>Lag3</th><td>0.03319458  </td><td>-0.010803402</td><td>-0.025896670</td><td> 1.000000000</td><td>-0.024051036</td><td>-0.018808338</td><td>-0.04182369 </td><td>-0.002447647</td></tr>\n",
       "\t<tr><th scope=row>Lag4</th><td>0.03568872  </td><td>-0.002985911</td><td>-0.010853533</td><td>-0.024051036</td><td> 1.000000000</td><td>-0.027083641</td><td>-0.04841425 </td><td>-0.006899527</td></tr>\n",
       "\t<tr><th scope=row>Lag5</th><td>0.02978799  </td><td>-0.005674606</td><td>-0.003557949</td><td>-0.018808338</td><td>-0.027083641</td><td> 1.000000000</td><td>-0.02200231 </td><td>-0.034860083</td></tr>\n",
       "\t<tr><th scope=row>Volume</th><td>0.53900647  </td><td> 0.040909908</td><td>-0.043383215</td><td>-0.041823686</td><td>-0.048414246</td><td>-0.022002315</td><td> 1.00000000 </td><td> 0.014591823</td></tr>\n",
       "\t<tr><th scope=row>Today</th><td>0.03009523  </td><td>-0.026155045</td><td>-0.010250033</td><td>-0.002447647</td><td>-0.006899527</td><td>-0.034860083</td><td> 0.01459182 </td><td> 1.000000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllll}\n",
       "  & Year & Lag1 & Lag2 & Lag3 & Lag4 & Lag5 & Volume & Today\\\\\n",
       "\\hline\n",
       "\tYear & 1.00000000   &  0.029699649 &  0.030596422 &  0.033194581 &  0.035688718 &  0.029787995 &  0.53900647  &  0.030095229\\\\\n",
       "\tLag1 & 0.02969965   &  1.000000000 & -0.026294328 & -0.010803402 & -0.002985911 & -0.005674606 &  0.04090991  & -0.026155045\\\\\n",
       "\tLag2 & 0.03059642   & -0.026294328 &  1.000000000 & -0.025896670 & -0.010853533 & -0.003557949 & -0.04338321  & -0.010250033\\\\\n",
       "\tLag3 & 0.03319458   & -0.010803402 & -0.025896670 &  1.000000000 & -0.024051036 & -0.018808338 & -0.04182369  & -0.002447647\\\\\n",
       "\tLag4 & 0.03568872   & -0.002985911 & -0.010853533 & -0.024051036 &  1.000000000 & -0.027083641 & -0.04841425  & -0.006899527\\\\\n",
       "\tLag5 & 0.02978799   & -0.005674606 & -0.003557949 & -0.018808338 & -0.027083641 &  1.000000000 & -0.02200231  & -0.034860083\\\\\n",
       "\tVolume & 0.53900647   &  0.040909908 & -0.043383215 & -0.041823686 & -0.048414246 & -0.022002315 &  1.00000000  &  0.014591823\\\\\n",
       "\tToday & 0.03009523   & -0.026155045 & -0.010250033 & -0.002447647 & -0.006899527 & -0.034860083 &  0.01459182  &  1.000000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Year | Lag1 | Lag2 | Lag3 | Lag4 | Lag5 | Volume | Today |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| Year | 1.00000000   |  0.029699649 |  0.030596422 |  0.033194581 |  0.035688718 |  0.029787995 |  0.53900647  |  0.030095229 |\n",
       "| Lag1 | 0.02969965   |  1.000000000 | -0.026294328 | -0.010803402 | -0.002985911 | -0.005674606 |  0.04090991  | -0.026155045 |\n",
       "| Lag2 | 0.03059642   | -0.026294328 |  1.000000000 | -0.025896670 | -0.010853533 | -0.003557949 | -0.04338321  | -0.010250033 |\n",
       "| Lag3 | 0.03319458   | -0.010803402 | -0.025896670 |  1.000000000 | -0.024051036 | -0.018808338 | -0.04182369  | -0.002447647 |\n",
       "| Lag4 | 0.03568872   | -0.002985911 | -0.010853533 | -0.024051036 |  1.000000000 | -0.027083641 | -0.04841425  | -0.006899527 |\n",
       "| Lag5 | 0.02978799   | -0.005674606 | -0.003557949 | -0.018808338 | -0.027083641 |  1.000000000 | -0.02200231  | -0.034860083 |\n",
       "| Volume | 0.53900647   |  0.040909908 | -0.043383215 | -0.041823686 | -0.048414246 | -0.022002315 |  1.00000000  |  0.014591823 |\n",
       "| Today | 0.03009523   | -0.026155045 | -0.010250033 | -0.002447647 | -0.006899527 | -0.034860083 |  0.01459182  |  1.000000000 |\n",
       "\n"
      ],
      "text/plain": [
       "       Year       Lag1         Lag2         Lag3         Lag4        \n",
       "Year   1.00000000  0.029699649  0.030596422  0.033194581  0.035688718\n",
       "Lag1   0.02969965  1.000000000 -0.026294328 -0.010803402 -0.002985911\n",
       "Lag2   0.03059642 -0.026294328  1.000000000 -0.025896670 -0.010853533\n",
       "Lag3   0.03319458 -0.010803402 -0.025896670  1.000000000 -0.024051036\n",
       "Lag4   0.03568872 -0.002985911 -0.010853533 -0.024051036  1.000000000\n",
       "Lag5   0.02978799 -0.005674606 -0.003557949 -0.018808338 -0.027083641\n",
       "Volume 0.53900647  0.040909908 -0.043383215 -0.041823686 -0.048414246\n",
       "Today  0.03009523 -0.026155045 -0.010250033 -0.002447647 -0.006899527\n",
       "       Lag5         Volume      Today       \n",
       "Year    0.029787995  0.53900647  0.030095229\n",
       "Lag1   -0.005674606  0.04090991 -0.026155045\n",
       "Lag2   -0.003557949 -0.04338321 -0.010250033\n",
       "Lag3   -0.018808338 -0.04182369 -0.002447647\n",
       "Lag4   -0.027083641 -0.04841425 -0.006899527\n",
       "Lag5    1.000000000 -0.02200231 -0.034860083\n",
       "Volume -0.022002315  1.00000000  0.014591823\n",
       "Today  -0.034860083  0.01459182  1.000000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cor(Smarket[, -9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the last row, we see that the correlations between the lag variables and today's returns are close to zero, which means that there appears to be little correlation between today's returns and the returns from previous days. The only substantial correlation is between `Year` and `Volume`, with a value of 0.539. We can plot the data to see that `Volume` is increasing over time. In other words, the average number of shares traded daily increased from 2001 to 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAMFBMVEUAAABNTU1oaGh8fHyM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\nRK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(Smarket$Volume, xlab = \"Day\", ylab = \"Shares traded (in billions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start off by fitting a logistic regression model in order to predict `Direction` using `Lag1` through `Lag5` and `Volume`. To do this, we'll use the `glm()` function, which fits *generalized linear models*, which is a class of models that includes linear regression. While the syntax for `glm()` is quite similar to that of `lm()`, we must pass in the additional argument `family` in order to tell `R` what kind of generalize linear model we are using. For logistic regression, we use `family = binomial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Direction ~ . - Year - Today, family = binomial, \n",
       "    data = Smarket)\n",
       "\n",
       "Deviance Residuals: \n",
       "   Min      1Q  Median      3Q     Max  \n",
       "-1.446  -1.203   1.065   1.145   1.326  \n",
       "\n",
       "Coefficients:\n",
       "             Estimate Std. Error z value Pr(>|z|)\n",
       "(Intercept) -0.126000   0.240736  -0.523    0.601\n",
       "Lag1        -0.073074   0.050167  -1.457    0.145\n",
       "Lag2        -0.042301   0.050086  -0.845    0.398\n",
       "Lag3         0.011085   0.049939   0.222    0.824\n",
       "Lag4         0.009359   0.049974   0.187    0.851\n",
       "Lag5         0.010313   0.049511   0.208    0.835\n",
       "Volume       0.135441   0.158360   0.855    0.392\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 1731.2  on 1249  degrees of freedom\n",
       "Residual deviance: 1727.6  on 1243  degrees of freedom\n",
       "AIC: 1741.6\n",
       "\n",
       "Number of Fisher Scoring iterations: 3\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glm.fits = glm(Direction ~ . - Year - Today, data = Smarket, family = binomial)\n",
    "summary(glm.fits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `Lag1` has the smallest p-value among all of the coefficient estimates, it still has a value of 0.145, which is still relatively large. Thus, even though the negative coefficient for `Lag1` suggests that if the market had a positive return yesterday, then it is less likely to go up today, there isn't any clear evidence of a real association between `Lag1` and `Direction`.\n",
    "\n",
    "As discussed in Lab 2, we can use the `coef()` function to access just the coefficients of the fitted model. In addition, we can also use the `summary()` function to access particular aspects of the fitted model, such as the p-values for the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>(Intercept)</dt>\n",
       "\t\t<dd>-0.126000256559266</dd>\n",
       "\t<dt>Lag1</dt>\n",
       "\t\t<dd>-0.0730737458900261</dd>\n",
       "\t<dt>Lag2</dt>\n",
       "\t\t<dd>-0.0423013440073083</dd>\n",
       "\t<dt>Lag3</dt>\n",
       "\t\t<dd>0.0110851083796762</dd>\n",
       "\t<dt>Lag4</dt>\n",
       "\t\t<dd>0.00935893837027871</dd>\n",
       "\t<dt>Lag5</dt>\n",
       "\t\t<dd>0.0103130684758179</dd>\n",
       "\t<dt>Volume</dt>\n",
       "\t\t<dd>0.13544065885916</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[(Intercept)] -0.126000256559266\n",
       "\\item[Lag1] -0.0730737458900261\n",
       "\\item[Lag2] -0.0423013440073083\n",
       "\\item[Lag3] 0.0110851083796762\n",
       "\\item[Lag4] 0.00935893837027871\n",
       "\\item[Lag5] 0.0103130684758179\n",
       "\\item[Volume] 0.13544065885916\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "(Intercept)\n",
       ":   -0.126000256559266Lag1\n",
       ":   -0.0730737458900261Lag2\n",
       ":   -0.0423013440073083Lag3\n",
       ":   0.0110851083796762Lag4\n",
       ":   0.00935893837027871Lag5\n",
       ":   0.0103130684758179Volume\n",
       ":   0.13544065885916\n",
       "\n"
      ],
      "text/plain": [
       " (Intercept)         Lag1         Lag2         Lag3         Lag4         Lag5 \n",
       "-0.126000257 -0.073073746 -0.042301344  0.011085108  0.009358938  0.010313068 \n",
       "      Volume \n",
       " 0.135440659 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coef(glm.fits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Estimate</th><th scope=col>Std. Error</th><th scope=col>z value</th><th scope=col>Pr(&gt;|z|)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>(Intercept)</th><td>-0.126000257</td><td>0.24073574  </td><td>-0.5233966  </td><td>0.6006983   </td></tr>\n",
       "\t<tr><th scope=row>Lag1</th><td>-0.073073746</td><td>0.05016739  </td><td>-1.4565986  </td><td>0.1452272   </td></tr>\n",
       "\t<tr><th scope=row>Lag2</th><td>-0.042301344</td><td>0.05008605  </td><td>-0.8445733  </td><td>0.3983491   </td></tr>\n",
       "\t<tr><th scope=row>Lag3</th><td> 0.011085108</td><td>0.04993854  </td><td> 0.2219750  </td><td>0.8243333   </td></tr>\n",
       "\t<tr><th scope=row>Lag4</th><td> 0.009358938</td><td>0.04997413  </td><td> 0.1872757  </td><td>0.8514445   </td></tr>\n",
       "\t<tr><th scope=row>Lag5</th><td> 0.010313068</td><td>0.04951146  </td><td> 0.2082966  </td><td>0.8349974   </td></tr>\n",
       "\t<tr><th scope=row>Volume</th><td> 0.135440659</td><td>0.15835970  </td><td> 0.8552723  </td><td>0.3924004   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & Estimate & Std. Error & z value & Pr(>\\textbar{}z\\textbar{})\\\\\n",
       "\\hline\n",
       "\t(Intercept) & -0.126000257 & 0.24073574   & -0.5233966   & 0.6006983   \\\\\n",
       "\tLag1 & -0.073073746 & 0.05016739   & -1.4565986   & 0.1452272   \\\\\n",
       "\tLag2 & -0.042301344 & 0.05008605   & -0.8445733   & 0.3983491   \\\\\n",
       "\tLag3 &  0.011085108 & 0.04993854   &  0.2219750   & 0.8243333   \\\\\n",
       "\tLag4 &  0.009358938 & 0.04997413   &  0.1872757   & 0.8514445   \\\\\n",
       "\tLag5 &  0.010313068 & 0.04951146   &  0.2082966   & 0.8349974   \\\\\n",
       "\tVolume &  0.135440659 & 0.15835970   &  0.8552723   & 0.3924004   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Estimate | Std. Error | z value | Pr(>|z|) |\n",
       "|---|---|---|---|---|\n",
       "| (Intercept) | -0.126000257 | 0.24073574   | -0.5233966   | 0.6006983    |\n",
       "| Lag1 | -0.073073746 | 0.05016739   | -1.4565986   | 0.1452272    |\n",
       "| Lag2 | -0.042301344 | 0.05008605   | -0.8445733   | 0.3983491    |\n",
       "| Lag3 |  0.011085108 | 0.04993854   |  0.2219750   | 0.8243333    |\n",
       "| Lag4 |  0.009358938 | 0.04997413   |  0.1872757   | 0.8514445    |\n",
       "| Lag5 |  0.010313068 | 0.04951146   |  0.2082966   | 0.8349974    |\n",
       "| Volume |  0.135440659 | 0.15835970   |  0.8552723   | 0.3924004    |\n",
       "\n"
      ],
      "text/plain": [
       "            Estimate     Std. Error z value    Pr(>|z|) \n",
       "(Intercept) -0.126000257 0.24073574 -0.5233966 0.6006983\n",
       "Lag1        -0.073073746 0.05016739 -1.4565986 0.1452272\n",
       "Lag2        -0.042301344 0.05008605 -0.8445733 0.3983491\n",
       "Lag3         0.011085108 0.04993854  0.2219750 0.8243333\n",
       "Lag4         0.009358938 0.04997413  0.1872757 0.8514445\n",
       "Lag5         0.010313068 0.04951146  0.2082966 0.8349974\n",
       "Volume       0.135440659 0.15835970  0.8552723 0.3924004"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(glm.fits)$coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>(Intercept)</dt>\n",
       "\t\t<dd>0.600698319413356</dd>\n",
       "\t<dt>Lag1</dt>\n",
       "\t\t<dd>0.145227211568647</dd>\n",
       "\t<dt>Lag2</dt>\n",
       "\t\t<dd>0.398349095427021</dd>\n",
       "\t<dt>Lag3</dt>\n",
       "\t\t<dd>0.824333346101536</dd>\n",
       "\t<dt>Lag4</dt>\n",
       "\t\t<dd>0.851444506926455</dd>\n",
       "\t<dt>Lag5</dt>\n",
       "\t\t<dd>0.834997390499829</dd>\n",
       "\t<dt>Volume</dt>\n",
       "\t\t<dd>0.39240043320243</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[(Intercept)] 0.600698319413356\n",
       "\\item[Lag1] 0.145227211568647\n",
       "\\item[Lag2] 0.398349095427021\n",
       "\\item[Lag3] 0.824333346101536\n",
       "\\item[Lag4] 0.851444506926455\n",
       "\\item[Lag5] 0.834997390499829\n",
       "\\item[Volume] 0.39240043320243\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "(Intercept)\n",
       ":   0.600698319413356Lag1\n",
       ":   0.145227211568647Lag2\n",
       ":   0.398349095427021Lag3\n",
       ":   0.824333346101536Lag4\n",
       ":   0.851444506926455Lag5\n",
       ":   0.834997390499829Volume\n",
       ":   0.39240043320243\n",
       "\n"
      ],
      "text/plain": [
       "(Intercept)        Lag1        Lag2        Lag3        Lag4        Lag5 \n",
       "  0.6006983   0.1452272   0.3983491   0.8243333   0.8514445   0.8349974 \n",
       "     Volume \n",
       "  0.3924004 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(glm.fits)$coef[, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make a prediction, we use the `predict()` function to use the model to compute the probability that the market will go up for a set of given values of the predictors. Note that we need to specify that `type = \"response\"` in order to tell `R` to output probabilities of the form $P(Y = 1 | X)$, as oppsed to other information souch as the logit. If we don't supply a data set to the `predict()` function, then it will compute the probabilties for the training data that was used to fit the model. Here are the first ten probabilities that `predict()` computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm.probs = predict(glm.fits, type = \"response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>1</dt>\n",
       "\t\t<dd>0.507084133395402</dd>\n",
       "\t<dt>2</dt>\n",
       "\t\t<dd>0.481467878454591</dd>\n",
       "\t<dt>3</dt>\n",
       "\t\t<dd>0.481138835214201</dd>\n",
       "\t<dt>4</dt>\n",
       "\t\t<dd>0.515222355813022</dd>\n",
       "\t<dt>5</dt>\n",
       "\t\t<dd>0.510781162691538</dd>\n",
       "\t<dt>6</dt>\n",
       "\t\t<dd>0.506956460534911</dd>\n",
       "\t<dt>7</dt>\n",
       "\t\t<dd>0.492650874187038</dd>\n",
       "\t<dt>8</dt>\n",
       "\t\t<dd>0.509229158207377</dd>\n",
       "\t<dt>9</dt>\n",
       "\t\t<dd>0.517613526170958</dd>\n",
       "\t<dt>10</dt>\n",
       "\t\t<dd>0.488837779771376</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[1] 0.507084133395402\n",
       "\\item[2] 0.481467878454591\n",
       "\\item[3] 0.481138835214201\n",
       "\\item[4] 0.515222355813022\n",
       "\\item[5] 0.510781162691538\n",
       "\\item[6] 0.506956460534911\n",
       "\\item[7] 0.492650874187038\n",
       "\\item[8] 0.509229158207377\n",
       "\\item[9] 0.517613526170958\n",
       "\\item[10] 0.488837779771376\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "1\n",
       ":   0.5070841333954022\n",
       ":   0.4814678784545913\n",
       ":   0.4811388352142014\n",
       ":   0.5152223558130225\n",
       ":   0.5107811626915386\n",
       ":   0.5069564605349117\n",
       ":   0.4926508741870388\n",
       ":   0.5092291582073779\n",
       ":   0.51761352617095810\n",
       ":   0.488837779771376\n",
       "\n"
      ],
      "text/plain": [
       "        1         2         3         4         5         6         7         8 \n",
       "0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 0.5069565 0.4926509 0.5092292 \n",
       "        9        10 \n",
       "0.5176135 0.4888378 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glm.probs[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values correspond to the probability of the market going up, rather than down, since `R` created a dummy variable with value 1 for `Up` and 0 for `Down`. We can verify this using the `contrasts()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Up</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>Down</th><td>0</td></tr>\n",
       "\t<tr><th scope=row>Up</th><td>1</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "  & Up\\\\\n",
       "\\hline\n",
       "\tDown & 0\\\\\n",
       "\tUp & 1\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Up |\n",
       "|---|---|\n",
       "| Down | 0 |\n",
       "| Up | 1 |\n",
       "\n"
      ],
      "text/plain": [
       "     Up\n",
       "Down 0 \n",
       "Up   1 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "contrasts(Smarket$Direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier for us to make predictions as to whether the market will go up or down on a particular day, we convert the predicted probabilities into class labels `Up` or `Down`. First, we create a vector of 1,250 `Down` entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm.pred = rep(\"Down\", 1250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use a boolean mask to change all of the elements for which the predicted probability of a market increase exceeds 0.5 from `Down` to `Up`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm.pred[glm.probs > 0.5] = \"Up\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these predictions in hand, we can use the `table()` function to produce a confusion matrix in order to determine how many observations were correctly or incorrectly labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        \n",
       "glm.pred Down  Up\n",
       "    Down  145 141\n",
       "    Up    457 507"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(glm.pred, Smarket$Direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal of the confusion matrix represents correct predictions, while the off-diagonal represents incorrect predictions. Thus, the model correctly predicted that the market would go up on 507 days and go down on 145 days, for a total of 652 correct prediction. This means that logistic regression correctly predicted the movement of the market $652/1250 = 52.16\\%$ of the time. Another way we can compute this is to use the `mean()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.5216"
      ],
      "text/latex": [
       "0.5216"
      ],
      "text/markdown": [
       "0.5216"
      ],
      "text/plain": [
       "[1] 0.5216"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(glm.pred == Smarket$Direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works because the `glm.pred == Smarket$Direction` is a boolean vector, and in the context of the `mean()` function the value `TRUE` is treated as 1 and `FALSE` is treated as 0.\n",
    "\n",
    "Note that this correct prediction rate of 52.16% comes from testing our logistic regression model on the same data set we used for training. In other words, our *training* error rate is 47.84. Since the training error rate tends to underestimate the test error rate, the correct prediction rate isn't as good as it might initially seem. One way we can better assess the accuracy of the logistic regression model in this setting is to fif the model using part of the data, and then examine how well it performs when predicting the *held out* data. Since in practice we are more interested with how well our model performs on days in the future for which the market's movements are unknown, this will yield a more realistic error rate. In order to implement this strategy, we'll again use a boolean mask to separate out the observations from 2001 through 2004 from the observations from 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = (Smarket$Year < 2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we create a boolean mask where each element is either `TRUE` if it corresponds to an observation that occurred before 2005 or `FALSE` if it corresponds to an observation that occurred in 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Smarket.2005 = Smarket[!train, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the mask to pick out the submatrix of the stock market data set which consists of all columns, and only the rows for which the elements of `train` are false. In other words, `Smarket.2005` is a copy of the submatrix of `Smarket` consisting only of all observations from 2005. Note that we didn't need to create the mask `train` separately, but doing so is good practice to allow us to easily manipulate our training set in a single place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "TRUE"
      ],
      "text/latex": [
       "TRUE"
      ],
      "text/markdown": [
       "TRUE"
      ],
      "text/plain": [
       "[1] TRUE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "identical(Smarket.2005, Smarket[!(Smarket$Year < 2005), ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, creating the mask separately saves us a little bit of typing when fitting a logistic regresison model using only the subset of the observations that correspond to dates before 2005. To do so, we use the `subset` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm.fits = glm(Direction ~ . - Year - Today, data = Smarket, family = binomial, subset = train)\n",
    "glm.probs = predict(glm.fits, Smarket.2005, type = \"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we emphasize that we trained and tested our model on two completely separate data sets: we used only the dates from before 2005 for training, and only used dates from 2005 for testing. Again lets compute the predictions for 2005 and compare them to the actual movements of the market over that time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        \n",
       "glm.pred Down Up\n",
       "    Down   77 97\n",
       "    Up     34 44"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glm.pred = rep(\"Down\", dim(Smarket.2005)[1])\n",
    "glm.pred[glm.probs > 0.5] = \"Up\"\n",
    "table(glm.pred, Smarket.2005$Direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.48015873015873"
      ],
      "text/latex": [
       "0.48015873015873"
      ],
      "text/markdown": [
       "0.48015873015873"
      ],
      "text/plain": [
       "[1] 0.4801587"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(glm.pred == Smarket.2005$Direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.51984126984127"
      ],
      "text/latex": [
       "0.51984126984127"
      ],
      "text/markdown": [
       "0.51984126984127"
      ],
      "text/plain": [
       "[1] 0.5198413"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(glm.pred != Smarket.2005$Direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that when we used separate training and testing sets the test error rate is about 52%, which is worse than random guessing! This suggests that we should try to experiment some more to see if we can improve our model. One thing we can try is removing the variables that didn't seem to be helpful in predicting `Direction` and refit the logistic regression model. As we recall, none of the p-values for the predictors were particularly small, even the smallest one which corresponded to `Lag1`. Since using predictors that have no relationship with the response increase the variance of a model without a corresponding decrease in bias, they therefore tend to negatively impact the test error rate. Let's try refitting the logistic regression using just `Lag1` and `Lag2`, which seemed to have the highest predictive power in the original logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        \n",
       "glm.pred Down  Up\n",
       "    Down   35  35\n",
       "    Up     76 106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glm.fits = glm(Direction ~ Lag1 + Lag2, data = Smarket, family = binomial, subset = train)\n",
    "glm.probs = predict(glm.fits, Smarket.2005, type = \"response\")\n",
    "glm.pred = rep(\"Down\", dim(Smarket.2005)[1])\n",
    "glm.pred[glm.probs > 0.5] = \"Up\"\n",
    "table(glm.pred, Smarket.2005$Direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.55952380952381"
      ],
      "text/latex": [
       "0.55952380952381"
      ],
      "text/markdown": [
       "0.55952380952381"
      ],
      "text/plain": [
       "[1] 0.5595238"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(glm.pred == Smarket.2005$Direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here things look a little better, since about 56% of the daily movements in 2005 were correctly predicted. However, if we predicted that the market would increase every day we would also have the same error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.55952380952381"
      ],
      "text/latex": [
       "0.55952380952381"
      ],
      "text/markdown": [
       "0.55952380952381"
      ],
      "text/plain": [
       "[1] 0.5595238"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(Smarket.2005$Direction == \"Up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, in terms of the overall error rate, logistic regression is no better than naively predicting that the market would increase every day. However, when we examine the confusion matrix a little more closely we can see that on days when logistic regression predicts an increase in the maket, it has a slightly better accuracy rate of $106 / (106 + 76) = 0.582$. This suggests a possible strategy of buying on days when the model predicts an increasing market, and avoiding trades when the model predicts a decreasing market. Without further investigation, though, we do not know if this small improvement is real or just due to random chance.\n",
    "\n",
    "Lastly, if we wish to predict the returns associated with particular values of `Lag1` and `Lag2`, such as on a day when `Lag1 = 1.2` and `Lag2 = 1.1`, or on a day when `Lag1 = 1.5` and `Lag2 = -0.8`, we can do this using the `predict()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>1</dt>\n",
       "\t\t<dd>0.479146239171912</dd>\n",
       "\t<dt>2</dt>\n",
       "\t\t<dd>0.496093872956532</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[1] 0.479146239171912\n",
       "\\item[2] 0.496093872956532\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "1\n",
       ":   0.4791462391719122\n",
       ":   0.496093872956532\n",
       "\n"
      ],
      "text/plain": [
       "        1         2 \n",
       "0.4791462 0.4960939 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict(glm.fits, newdata = data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)), type = \"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll perform linear discriminant analysis (LDA) on the `Smarket` data. In order to do so, we'll need to load the `MASS` library in order to use the `lda()` function. Aside from the absense of the `family` option, the syntax for `lda()` is the same as that for `lm()` and `glm()`. Just as we did with the second attempt at using logistic regression, we'll only use the observations from before 2005 to fit the model, and we'll only use `Lag1` and `Lag2` as the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(MASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit = lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n",
       "\n",
       "Prior probabilities of groups:\n",
       "    Down       Up \n",
       "0.491984 0.508016 \n",
       "\n",
       "Group means:\n",
       "            Lag1        Lag2\n",
       "Down  0.04279022  0.03389409\n",
       "Up   -0.03954635 -0.03132544\n",
       "\n",
       "Coefficients of linear discriminants:\n",
       "            LD1\n",
       "Lag1 -0.6420190\n",
       "Lag2 -0.5135293"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lda.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we note that the LDA output indicates that the estimated prior probabilities of the groups are $\\hat{\\pi}_1 = 0.492$ and $\\hat{\\pi}_2 = 0.508$. This means that 49.2% of the training observations correspond to days during which the market went down and 50.8% of the training observations correspond to days during which the market went up. In addition, the output also provides the averages of each predictor within each class (e.g. the average value of `Lag1` for days in which the market went down is 0.0428), which are used in LDA as estimates of the actual group means $\\mu_k$. These suggest that there is a tendency for the previous two days' returns to be negative on the days when the market increases, and a tendency for the previous two days' returns to be positive on the days when the market decreases. Lastly, the *coefficients of linear discriminants* portion of the output provides the linear combination of `Lag1` and `Lag2` that are used to form the LDA decision rule. In other words, you can intuitively think that these are the multipliers of the elements of $X = x$ in the formula\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_k(x) = x^T\\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + \\log(\\pi_k).\n",
    "\\end{equation}\n",
    "\n",
    "However, it is important to note that the values given by `lda()` ***are not*** the actual multipliers of the elements of $X = x$ in the above formula, as [they are normalized so that the within groups covariance is spherical](https://www.rdocumentation.org/packages/MASS/versions/7.3-51.4/topics/lda). Instead they are scaled coefficients which are discussed more in detail in [this stats.stackexchange post](https://stats.stackexchange.com/a/48859).\n",
    "\n",
    "Recall that in the fomula $\\Sigma$ is the $2 \\times 2$ covariance matrix of `Lag1` and `Lag2`; $\\mu_k$ is the vector of the group means of `Lag1` and `Lag2` for either the group `Down` or the group `Up`; $\\pi_k$ is the estimated prior probability of either `Down` or `Up`. Strictly speaking, since we are using estimates in the model, $\\Sigma$, $\\mu_k$, and $\\pi_k$ should be replaced with their hatted versions. Also recall that this formula is the vector/matrix version of\n",
    "\n",
    "\\begin{equation}\n",
    "    \\delta_k = x\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k).\n",
    "\\end{equation}\n",
    "\n",
    "More concretely, if $-0.642 \\times$ `Lag1` $- 0.514 \\times$ `Lag2` is large, then the LDA classifier will predict a market increase; if it is small, then the LDA classifier will predict a market decline. We can produce plots of the *linear discriminants*, obtained by computing $-0.642 \\times$ `Lag1` $- 0.514 \\times$ `Lag2` for each of the training observations, using the `plot()` function on `lda.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAM1BMVEUAAAAA//9NTU1oaGh8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\ns08syQAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(lda.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `predict()` function on `lda.fit` returns a list with three elements.\n",
    "\n",
    "- `class` contains LDA's predictions about the movement of the market.\n",
    "- `posterior` is a matrix whose $k$th column contains the posterior probability that the corresponding observation belongs to the $k$th class. This is computed using Bayes' theorem.\n",
    "- `x` contains the linear discriminants, as described earlier.\n",
    "\n",
    "Recall that Bayes' theorem states that\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{Pr}(Y = k | X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l = 1}^K \\pi_l f_l(x)},\n",
    "\\end{equation}\n",
    "\n",
    "where $K$ is the total number of classes for $Y$; $\\pi_k$ is the overall, or *prior* probability that a randomly chosen observation comes from the $k$th class; and $f_k(x)$ is defined to be $\\text{Pr}(X = x | Y = k)$, the probability that $X = x$ for an observation $X$ that comes from the $k$th class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.pred = predict(lda.fit, Smarket.2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in Section 4.5 of the book, we saw that LDA and logistic regression often have similar performance. We see that this is the case for our stock market data. In fact, it just so happens that we have an identical confusion matrix and test accuracy number compared to the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         \n",
       "lda.class Down  Up\n",
       "     Down   35  35\n",
       "     Up     76 106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lda.class = lda.pred$class\n",
    "table(lda.class, Smarket.2005$Direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.55952380952381"
      ],
      "text/latex": [
       "0.55952380952381"
      ],
      "text/markdown": [
       "0.55952380952381"
      ],
      "text/plain": [
       "[1] 0.5595238"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(lda.class == Smarket.2005$Direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that since there are two classes, LDA uses a posterior probability threshold of 50% in order to make the predictions that go into `class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Down</th><th scope=col>Up</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>999</th><td>0.4901792</td><td>0.5098208</td></tr>\n",
       "\t<tr><th scope=row>1000</th><td>0.4792185</td><td>0.5207815</td></tr>\n",
       "\t<tr><th scope=row>1001</th><td>0.4668185</td><td>0.5331815</td></tr>\n",
       "\t<tr><th scope=row>1002</th><td>0.4740011</td><td>0.5259989</td></tr>\n",
       "\t<tr><th scope=row>1003</th><td>0.4927877</td><td>0.5072123</td></tr>\n",
       "\t<tr><th scope=row>1004</th><td>0.4938562</td><td>0.5061438</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & Down & Up\\\\\n",
       "\\hline\n",
       "\t999 & 0.4901792 & 0.5098208\\\\\n",
       "\t1000 & 0.4792185 & 0.5207815\\\\\n",
       "\t1001 & 0.4668185 & 0.5331815\\\\\n",
       "\t1002 & 0.4740011 & 0.5259989\\\\\n",
       "\t1003 & 0.4927877 & 0.5072123\\\\\n",
       "\t1004 & 0.4938562 & 0.5061438\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Down | Up |\n",
       "|---|---|---|\n",
       "| 999 | 0.4901792 | 0.5098208 |\n",
       "| 1000 | 0.4792185 | 0.5207815 |\n",
       "| 1001 | 0.4668185 | 0.5331815 |\n",
       "| 1002 | 0.4740011 | 0.5259989 |\n",
       "| 1003 | 0.4927877 | 0.5072123 |\n",
       "| 1004 | 0.4938562 | 0.5061438 |\n",
       "\n"
      ],
      "text/plain": [
       "     Down      Up       \n",
       "999  0.4901792 0.5098208\n",
       "1000 0.4792185 0.5207815\n",
       "1001 0.4668185 0.5331815\n",
       "1002 0.4740011 0.5259989\n",
       "1003 0.4927877 0.5072123\n",
       "1004 0.4938562 0.5061438"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(lda.pred$posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>Up</li>\n",
       "\t<li>Up</li>\n",
       "\t<li>Up</li>\n",
       "\t<li>Up</li>\n",
       "\t<li>Up</li>\n",
       "\t<li>Up</li>\n",
       "</ol>\n",
       "\n",
       "<details>\n",
       "\t<summary style=display:list-item;cursor:pointer>\n",
       "\t\t<strong>Levels</strong>:\n",
       "\t</summary>\n",
       "\t<ol class=list-inline>\n",
       "\t\t<li>'Down'</li>\n",
       "\t\t<li>'Up'</li>\n",
       "\t</ol>\n",
       "</details>"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item Up\n",
       "\\item Up\n",
       "\\item Up\n",
       "\\item Up\n",
       "\\item Up\n",
       "\\item Up\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\emph{Levels}: \\begin{enumerate*}\n",
       "\\item 'Down'\n",
       "\\item 'Up'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. Up\n",
       "2. Up\n",
       "3. Up\n",
       "4. Up\n",
       "5. Up\n",
       "6. Up\n",
       "\n",
       "\n",
       "\n",
       "**Levels**: 1. 'Down'\n",
       "2. 'Up'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] Up Up Up Up Up Up\n",
       "Levels: Down Up"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(lda.class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to use a posterior probability threshold other than 50% in order to make predictions, we can use a boolean mask with the matrix of posterior probabilities. For example, let's suppose we wish to predict a market decrease only if we are very certain that it will indeed decrease on that day -- say, if the posterior probability of a decrease is at least 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0"
      ],
      "text/latex": [
       "0"
      ],
      "text/markdown": [
       "0"
      ],
      "text/plain": [
       "[1] 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(lda.pred$posterior[, \"Down\"] > 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No days in 2005 meet that threshold. In fact, the greatest posterior probability of decrease in all of 2005 was 52.02%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.520234950535616"
      ],
      "text/latex": [
       "0.520234950535616"
      ],
      "text/markdown": [
       "0.520234950535616"
      ],
      "text/plain": [
       "[1] 0.520235"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max(lda.pred$posterior[, \"Down\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, the overall greatest posterior probability for all of 2005 is only 54.22%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.542213255451898"
      ],
      "text/latex": [
       "0.542213255451898"
      ],
      "text/markdown": [
       "0.542213255451898"
      ],
      "text/plain": [
       "[1] 0.5422133"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max(lda.pred$posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's fit a quadratic discriminant analysis (QDA) model to the Smarket data. QDA is also part of the `MASS` library as the `qda()` function. It uses the same sytax as that of `lda()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n",
       "\n",
       "Prior probabilities of groups:\n",
       "    Down       Up \n",
       "0.491984 0.508016 \n",
       "\n",
       "Group means:\n",
       "            Lag1        Lag2\n",
       "Down  0.04279022  0.03389409\n",
       "Up   -0.03954635 -0.03132544"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qda.fit = qda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n",
    "qda.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the output for the result of `qda()` is similar to that as `lda()` in that it includes the prior probabilities of the groups as well as the group means, it does not contain the coefficients of the linear discriminants. This is because the QDA classifier involves a quadratic, not linear, function of the predictors. Recall that in QDA the formula $\\delta_k(x)$, the posterior probability that the observation $X = x$ is a member of the $k$th class, is\n",
    "\n",
    "\\begin{align}\n",
    "    \\delta_k(x) &= -\\frac{1}{2}(x - \\mu_k)^T\\Sigma_k^{-1}(x - \\mu_k) - \\frac{1}{2}\\log|\\Sigma_k| + \\log(\\pi_k) \\\\\n",
    "    &= -\\frac{1}{2}x^T\\Sigma_K^{-1}x + x^T\\Sigma_k^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma_k^{-1}\\mu_k - \\frac{1}{2}\\log|\\Sigma_k| + \\log(\\pi_k),\n",
    "\\end{align}\n",
    "\n",
    "where observations from the $k$th class are normally distributed with (population) mean $\\mu_k$ and (population) covariance matrix $\\Sigma_k$. This comes from the assumption in QDA that each class has its own covariance matrix, which differs from the assumption in LDA that the classes had the same covariance matrix.\n",
    "\n",
    "Even though the output for `qda()` is slightly different, the `predict()` function works in exactly the same fashion as for LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         \n",
       "qda.class Down  Up\n",
       "     Down   30  20\n",
       "     Up     81 121"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qda.pred = predict(qda.fit, Smarket.2005)\n",
    "qda.class = qda.pred$class\n",
    "table(qda.class, Smarket.2005$Direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.599206349206349"
      ],
      "text/latex": [
       "0.599206349206349"
      ],
      "text/markdown": [
       "0.599206349206349"
      ],
      "text/plain": [
       "[1] 0.5992063"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(qda.class == Smarket.2005$Direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that the QDA predictions are accurate almost 60% of the time for the 2005 data, which wasn't used to fit the model. This suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression. However, this model should be evaluated on a larger test set to get a better sense of its predictive strength.\n",
    "\n",
    "As a side note, we can again use a boolean mask with the matrix of posterior probabilities to make predictions with a probability threshold that is different from 50%. Let's see what QDA says for the cautious scenario of using a 90% probability threshold to predict a decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0"
      ],
      "text/latex": [
       "0"
      ],
      "text/markdown": [
       "0"
      ],
      "text/plain": [
       "[1] 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(qda.pred$posterior[, \"Down\"] > 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again no days in 2005 meet this threshold, since the maximum posterior probability of a decrease in all of 2005 was just 52.24%, and the overall maximum posterior probability in all of 2005 was 54.62%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.522407186549673"
      ],
      "text/latex": [
       "0.522407186549673"
      ],
      "text/markdown": [
       "0.522407186549673"
      ],
      "text/plain": [
       "[1] 0.5224072"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max(qda.pred$posterior[, \"Down\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.54618485050522"
      ],
      "text/latex": [
       "0.54618485050522"
      ],
      "text/markdown": [
       "0.54618485050522"
      ],
      "text/plain": [
       "[1] 0.5461849"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max(qda.pred$posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last method of working with the `Smarket` data we'll use is K-nearest neighbors. To do this, we'll use the `knn()` function from the `class` library. This function works differently from the other model-fitting functions that we have encountered thus far since it forms predictions with a single command instead of the two-step approach of first fitting the model and then using it to make predictions. The function requires four inputs.\n",
    "\n",
    "1. `train.X` is the matrix containing the predictors associated with the training data.\n",
    "2. `test.X` is the matrix containing the predictors associated with the data for which we wish to make predictions.\n",
    "3. `train.Direction` is a vector containing the class labels for the training observations.\n",
    "4. `k` is the number of nearest neighbors to be used by the classifier.\n",
    "\n",
    "We'll use `cbind()`, which is short for *column bind* to bind the `Lag1` and `Lag2` variables into our `train.X` and `test.X` matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.X = cbind(Smarket$Lag1, Smarket$Lag2)[train, ]\n",
    "test.X = cbind(Smarket$Lag1, Smarket$Lag2)[!train, ]\n",
    "train.Direction = Smarket$Direction[train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this setup step, we use the `knn()` function to predict the market's movement for the dates in 2005. Note that in order to have consistency and have reproducibility for our results, we set a random seed before applying `knn()` since `R` randomly breaks any ties if several observations are tied as nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        \n",
       "knn.pred Down Up\n",
       "    Down   43 58\n",
       "    Up     68 83"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(1)\n",
    "knn.pred = knn(train.X, test.X, train.Direction, k = 1)\n",
    "table(knn.pred, Smarket.2005$Direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $K = 1$, our results aren't very good, since only $(43 + 83)/252 = 50\\%$ of the observations are correctly predicted. Of course, using $K=1$ is a very flexible fit, so perhaps it was too much flexibility. Let's try again using $K = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        \n",
       "knn.pred Down Up\n",
       "    Down   48 54\n",
       "    Up     63 87"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knn.pred = knn(train.X, test.X, train.Direction, k = 3)\n",
    "table(knn.pred, Smarket.2005$Direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.535714285714286"
      ],
      "text/latex": [
       "0.535714285714286"
      ],
      "text/markdown": [
       "0.535714285714286"
      ],
      "text/plain": [
       "[1] 0.5357143"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(knn.pred == Smarket.2005$Direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results improved slightly to about 53.4% test accuracy, though further increasing $K$ does not provide any additional improvements. For this data, it appears that QDA provides the best results of the methods we have examined thus far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Application to Caravan Insurance Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll use KNN with the `Caravan` data set from the `ISLR` library. This set includes 85 predictors that measure demographic characteristics for 5,822 individuals. The response variable is `Purchase` which indicates whether or not a given individual purchases a caravan insurance policy. Again let's export a copy of this data to a CSV file for use when I do this lab in Python, and then explore it a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "write.csv(Caravan, \"Caravan.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this set, only about 6% of people purchased caravan insurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>No</dt>\n",
       "\t\t<dd>5474</dd>\n",
       "\t<dt>Yes</dt>\n",
       "\t\t<dd>348</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[No] 5474\n",
       "\\item[Yes] 348\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "No\n",
       ":   5474Yes\n",
       ":   348\n",
       "\n"
      ],
      "text/plain": [
       "  No  Yes \n",
       "5474  348 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(Caravan$Purchase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, we note that since the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Variables with a large scale will have a much larger effect on the *distance* between the observations, and hence on the KNN classifier, than variables that are on a small scale. For example, if we have a data set with the variables `salary` (measured in dollars) and `age` (measured in years), a difference of \\\\$1000 in salary is quite large compared to a difference of 50 years of age when it comes to computing distances in KNN. As a result, `salary` will dominate the KNN classification results while `age` will have almost no effect. Moreover, the importance of scale to the KNN classfier means that changing the units (such as measuring `salary` in Japanese yen, or measuring `age` in minutes) of the predictors could result in vastly different classification results compared to what we would get when using dollars and years.\n",
    "\n",
    "One way to handle this problem is to *standardize* the data so that all variables are given a mean of zero and a standard deviation of one. Doing this will put all variables on a comparable scale. While we could do this by hand, it is much more convenient to use the `scale()` function. When doing this with the `Caravan` data, we exclude column 86, the qualitative `Purchase` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized.X = scale(Caravan[,-86])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the mean and variance of first two columns of `Caravan` and compare those with that of the standardized columns to confirm that the standardized ones have mean zero and standard deviation one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "24.2533493644796"
      ],
      "text/latex": [
       "24.2533493644796"
      ],
      "text/markdown": [
       "24.2533493644796"
      ],
      "text/plain": [
       "[1] 24.25335"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(Caravan[, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "-7.02557607244499e-17"
      ],
      "text/latex": [
       "-7.02557607244499e-17"
      ],
      "text/markdown": [
       "-7.02557607244499e-17"
      ],
      "text/plain": [
       "[1] -7.025576e-17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(standardized.X[, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "1.11061490896599"
      ],
      "text/latex": [
       "1.11061490896599"
      ],
      "text/markdown": [
       "1.11061490896599"
      ],
      "text/plain": [
       "[1] 1.110615"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(Caravan[, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "-1.4708382205383e-16"
      ],
      "text/latex": [
       "-1.4708382205383e-16"
      ],
      "text/markdown": [
       "-1.4708382205383e-16"
      ],
      "text/plain": [
       "[1] -1.470838e-16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(standardized.X[, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the standardized means aren't exactly equal to zero due to slight floating point precision errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>MOSTYPE</th><th scope=col>MAANTHUI</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>MOSTYPE</th><td>165.0378474</td><td>-0.2018823 </td></tr>\n",
       "\t<tr><th scope=row>MAANTHUI</th><td> -0.2018823</td><td> 0.1647078 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & MOSTYPE & MAANTHUI\\\\\n",
       "\\hline\n",
       "\tMOSTYPE & 165.0378474 & -0.2018823 \\\\\n",
       "\tMAANTHUI &  -0.2018823 &  0.1647078 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | MOSTYPE | MAANTHUI |\n",
       "|---|---|---|\n",
       "| MOSTYPE | 165.0378474 | -0.2018823  |\n",
       "| MAANTHUI |  -0.2018823 |  0.1647078  |\n",
       "\n"
      ],
      "text/plain": [
       "         MOSTYPE     MAANTHUI  \n",
       "MOSTYPE  165.0378474 -0.2018823\n",
       "MAANTHUI  -0.2018823  0.1647078"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var(Caravan[, 1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>MOSTYPE</th><th scope=col>MAANTHUI</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>MOSTYPE</th><td> 1.00000000</td><td>-0.03872126</td></tr>\n",
       "\t<tr><th scope=row>MAANTHUI</th><td>-0.03872126</td><td> 1.00000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       "  & MOSTYPE & MAANTHUI\\\\\n",
       "\\hline\n",
       "\tMOSTYPE &  1.00000000 & -0.03872126\\\\\n",
       "\tMAANTHUI & -0.03872126 &  1.00000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | MOSTYPE | MAANTHUI |\n",
       "|---|---|---|\n",
       "| MOSTYPE |  1.00000000 | -0.03872126 |\n",
       "| MAANTHUI | -0.03872126 |  1.00000000 |\n",
       "\n"
      ],
      "text/plain": [
       "         MOSTYPE     MAANTHUI   \n",
       "MOSTYPE   1.00000000 -0.03872126\n",
       "MAANTHUI -0.03872126  1.00000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "var(standardized.X[, 1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the observations into a test set, containing the first 1,000 observations, and a training set, containing the remaining observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 1:1000\n",
    "train.X = standardized.X[-test, ]\n",
    "test.X = standardized.X[test, ]\n",
    "train.Y = Caravan$Purchase[-test]\n",
    "test.Y = Caravan$Purchase[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have separated out the training and test observations, we start off by fitting a KNN model on the training data using $K = 1$ and then evaluate is performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1)\n",
    "knn.pred = knn(train.X, test.X, train.Y, k = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.118"
      ],
      "text/latex": [
       "0.118"
      ],
      "text/markdown": [
       "0.118"
      ],
      "text/plain": [
       "[1] 0.118"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(test.Y != knn.pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KNN error rate with $K = 1$ is just under 12%, which appears to be pretty good at a first glance. However, since only 6% of customers purchased insurance, we could get the error rate down to 6% just by always predicting `No` regardless of the values of the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.059"
      ],
      "text/latex": [
       "0.059"
      ],
      "text/markdown": [
       "0.059"
      ],
      "text/plain": [
       "[1] 0.059"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean(test.Y != \"No\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the overall error rate is rather low, depending on the situation we may be interested in other methods of measuring the quality of the model. For example, if there is some non-trivial cost to trying to sell insurance to a given individual, then the company would like to try and sell insurance only to customers who are likely to buy it. In that case, the company is more interested in the fraction of individuals that are correctly predicted to buy insurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        test.Y\n",
       "knn.pred  No Yes\n",
       "     No  873  50\n",
       "     Yes  68   9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "table(knn.pred, test.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the fraction of individuals that are correctly predicted to buy insurance, the KNN classifier with $K = 1$ does far better than random guessing among the customers that are predicted to buy insurance. Among the 77 customers it predicted to buy insurance, 9 (11.7%) of them actually do purchase insurance. This is double the rate one would obtain from random guessing.\n",
    "\n",
    "We can also explore other values of $K$, such as $K = 3$ and $K = 5$, to see how performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        test.Y\n",
       "knn.pred  No Yes\n",
       "     No  920  54\n",
       "     Yes  21   5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knn.pred = knn(train.X, test.X, train.Y, k = 3)\n",
    "table(knn.pred, test.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        test.Y\n",
       "knn.pred  No Yes\n",
       "     No  930  55\n",
       "     Yes  11   4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knn.pred = knn(train.X, test.X, train.Y, k = 5)\n",
    "table(knn.pred, test.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With $K = 3$, the success rate of correctly predicting customers who buy insurance increases to $5/26 = 19\\%$, and with $K = 5$ the rate is $4/15 = 26.7\\%$. That is over four times the rate that we get from random guessing! This suggests that KNN is finding some real patterns in a difficult data set.\n",
    "\n",
    "To compare, we can also fit a logistic regression model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"glm.fit: fitted probabilities numerically 0 or 1 occurred\""
     ]
    }
   ],
   "source": [
    "glm.fits = glm(Purchase ~ ., data = Caravan, family = binomial, subset = -test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        test.Y\n",
       "glm.pred  No Yes\n",
       "     No  934  59\n",
       "     Yes   7   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glm.probs = predict(glm.fits, Caravan[test, ], type = \"response\")\n",
    "glm.pred = rep(\"No\", 1000)\n",
    "glm.pred[glm.probs > 0.5] = \"Yes\"\n",
    "table(glm.pred, test.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, if we use 0.5 as the predicted probability cut-off for logistic regression, only seven of the test observations are predicted to purchase insurance. Even worse, all seven of these predictions are incorrect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        test.Y\n",
       "glm.pred  No Yes\n",
       "     No  919  48\n",
       "     Yes  22  11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glm.probs = predict(glm.fits, Caravan[test, ], type = \"response\")\n",
    "glm.pred = rep(\"No\", 1000)\n",
    "glm.pred[glm.probs > 0.25] = \"Yes\"\n",
    "table(glm.pred, test.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aren't required to use a cut-off of 0.5, though. If we instead use a cut-off of 0.25, we get much better results: we predict that 33 people will purchase insurance, and we are correct for 33% of these people. This is over five times better than random guessing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
