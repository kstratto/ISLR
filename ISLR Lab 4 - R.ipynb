{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Cross-Validation and the Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll explore the four resampling techniques discussed in Chapter 5 of ISLR: the validation set approach, leave-one-out cross-validation (LOOCV), $k$-fold cross-validation, and the bootstrap. Note that some of the commands in this lab may take a while to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Validation Set Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll explore using the validation set approach to estimate the test error rates that result from fitting various linear models with the `Auto` data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ISLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that before starting, we use the `set.seed()` function in order to set a *seed* for `R`'s random number generator. This is generally a good idea to do when performing analyses that contain an element of randomness, such as cross-validation, in order to have reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we use the `sample()` function to split the set of observations into two halves by randomly selecting a training set that consists of 196 of the original 392 observations. Note that in passing the integer 392 as the first argument, `x` in sample, we are using the shortcut that sampling will take place from the vector `1:392`, as described in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sample(392, 196)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created the vector to denote our training set, we then use the `subset` option in `lm()` to fit a linear regression model using only the observations corresponding to those in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit = lm(mpg ~ horsepower, data = Auto, subset = train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting the linear regression of `mpg` onto `horsepower` using the training set, we use the `predict()` function to estimate the response for all 392 observations. Then, we use the `mean()` function to compute the mean squared error of the 196 observations in the validation set. Recall that the index `-train` below tells `R` to exclude the observations that are not in the training set. In other words, it selets those observations that are in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "23.2660086465003"
      ],
      "text/latex": [
       "23.2660086465003"
      ],
      "text/markdown": [
       "23.2660086465003"
      ],
      "text/plain": [
       "[1] 23.26601"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean((Auto$mpg - predict(lm.fit, Auto))[-train]^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the the estimated test mean squared error for the linear regression fit is 23.27. We can then use the `poly()` function to estimate the test error for the quadratic and cubic regressions. (As a side note, don't forget that by default the `poly()` function produces orthogonal polynomials by default. This isn't important for making predictions or estimating the test error, but it is still good to remember, since ISLR doesn't make this fact explicit.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "18.7164594933828"
      ],
      "text/latex": [
       "18.7164594933828"
      ],
      "text/markdown": [
       "18.7164594933828"
      ],
      "text/plain": [
       "[1] 18.71646"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm.fit2 = lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)\n",
    "mean((Auto$mpg - predict(lm.fit2, Auto))[-train]^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "18.7940067973945"
      ],
      "text/latex": [
       "18.7940067973945"
      ],
      "text/markdown": [
       "18.7940067973945"
      ],
      "text/plain": [
       "[1] 18.79401"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm.fit3 = lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)\n",
    "mean((Auto$mpg - predict(lm.fit3, Auto))[-train]^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the validation set mean squared error rates are 18.72 and 18.79 for the quadratic and cubic fits, respectively. Note that due to the element of randomness in choosing the training set, if we used a different seed (and therefore choose a possibly different training set), we will obtain somewhat different validation set error values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(2)\n",
    "train = sample(392, 196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "25.7265106448139"
      ],
      "text/latex": [
       "25.7265106448139"
      ],
      "text/markdown": [
       "25.7265106448139"
      ],
      "text/plain": [
       "[1] 25.72651"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm.fit = lm(mpg ~ horsepower, data = Auto, subset = train)\n",
    "mean((Auto$mpg - predict(lm.fit, Auto))[-train]^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "20.4303642741463"
      ],
      "text/latex": [
       "20.4303642741463"
      ],
      "text/markdown": [
       "20.4303642741463"
      ],
      "text/plain": [
       "[1] 20.43036"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm.fit2 = lm(mpg ~ poly(horsepower, 2), data = Auto, subset = train)\n",
    "mean((Auto$mpg - predict(lm.fit2, Auto))[-train]^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "20.3853268638776"
      ],
      "text/latex": [
       "20.3853268638776"
      ],
      "text/markdown": [
       "20.3853268638776"
      ],
      "text/plain": [
       "[1] 20.38533"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm.fit3 = lm(mpg ~ poly(horsepower, 3), data = Auto, subset = train)\n",
    "mean((Auto$mpg - predict(lm.fit3, Auto))[-train]^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this training set/validation set split, we get validation set mean squared error values of 25.73, 20.43, and 20.38 for the linear, quadratic, and cubic regression models, respectively. \n",
    "\n",
    "These results are consistent with our findings from the previous chapters that used an approach focused more on the statistics of the coefficient estimates: a quadratic model for predicting `mpg` using `horsepower` performs better than a linear model, and there isn't evidence to suggest that using a cubic model provides a meaningful improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up is leave-one-out cross validation (LOOCV). In `R` this can be automatically computed for any generalized linear model using the `glm()` and `cv.glm()` functions. Previously, in Lab 3, we used `glm()` to perform logistic regression by passing the argument `family = \"binomial\"`. If we don't pass this argument, then by default `glm()` performs linear regression just like the `lm()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>(Intercept)</dt>\n",
       "\t\t<dd>39.9358610211705</dd>\n",
       "\t<dt>horsepower</dt>\n",
       "\t\t<dd>-0.157844733353654</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[(Intercept)] 39.9358610211705\n",
       "\\item[horsepower] -0.157844733353654\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "(Intercept)\n",
       ":   39.9358610211705horsepower\n",
       ":   -0.157844733353654\n",
       "\n"
      ],
      "text/plain": [
       "(Intercept)  horsepower \n",
       " 39.9358610  -0.1578447 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glm.fit = glm(mpg ~ horsepower, data = Auto)\n",
    "coef(glm.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>(Intercept)</dt>\n",
       "\t\t<dd>39.9358610211705</dd>\n",
       "\t<dt>horsepower</dt>\n",
       "\t\t<dd>-0.157844733353654</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[(Intercept)] 39.9358610211705\n",
       "\\item[horsepower] -0.157844733353654\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "(Intercept)\n",
       ":   39.9358610211705horsepower\n",
       ":   -0.157844733353654\n",
       "\n"
      ],
      "text/plain": [
       "(Intercept)  horsepower \n",
       " 39.9358610  -0.1578447 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm.fit = lm(mpg ~ horsepower, data = Auto)\n",
    "coef(lm.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, both yield identical linear regression models. For this lab, we will use `glm()` over `lm()` in order to make use of the `cv.glm()` function, which is part of the `boot` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(boot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>24.2315135179292</li>\n",
       "\t<li>24.2311440937561</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 24.2315135179292\n",
       "\\item 24.2311440937561\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 24.2315135179292\n",
       "2. 24.2311440937561\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 24.23151 24.23114"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "glm.fit = glm(mpg ~ horsepower, data = Auto)\n",
    "cv.err = cv.glm(Auto, glm.fit)\n",
    "cv.err$delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cv.glm()` function produces a list with several components:\n",
    "\n",
    "- `call`: The original call to `cv.glm()`\n",
    "- `K`: The value of `K` used for the $k$-fold cross validation. If no value of `K` was supplied to the function call, then this is the number of observations in the matrix or data frame that was passed to the function.\n",
    "- `delta`: A vector of length two containing the cross-validation results. The first component is the raw cross-validation estimate of prediction error, which is given by the formula below. The second component is the adjusted cross-validation estimate, which uses an adjustment designed to compensate for the bias introduced if $k$-fold cross-validation is used instead of leave-one-out cross validation\n",
    "- `seed`: The value of `R`'s random seed when `cv.glm()` was called.\n",
    "\n",
    "Recall that the $k$-fold cross validation estimate for the test mean squared error with $k$ folds is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{CV}_{(k)} = \\frac{1}{k} \\sum_{i = 1}^k \\text{MSE}_i,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\text{MSE}_i$ is the mean squared error computed on the $i$th held out fold after fitting the model on the remaining $k - 1$ folds. In the case of leave-one-out cross-validation, $k = n$, the number of obserations in the data, and $\\text{MSE}_i$ is the mean squared error $(y_i - \\hat{y}_i)^2$ obtained after fitting the statistical learning method on all observations except for $y_i$.\n",
    "\n",
    "Since we used leave-one-out cross-validation in this call to the `cv.glm()` function, the two values in `delta` are equal up to two decimal places, with a value of 24.23.\n",
    "\n",
    "We can make use of a `for` loop to iteratively repeat the procedure of leave-one-out cross-validation for increasingly complex polynomial fits. We will iteratively fit polynomial regressions for polynomials of order $i = 1, \\dots, 10$, compute the associated LOOCV error, and store it in the $i$th element of the vector `cv.error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>24.2315135179292</li>\n",
       "\t<li>19.2482131244897</li>\n",
       "\t<li>19.3349840640291</li>\n",
       "\t<li>19.4244303104302</li>\n",
       "\t<li>19.0332138547041</li>\n",
       "\t<li>18.9786436582254</li>\n",
       "\t<li>18.8330450653182</li>\n",
       "\t<li>18.9611507120531</li>\n",
       "\t<li>19.0686299814602</li>\n",
       "\t<li>19.490932299329</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 24.2315135179292\n",
       "\\item 19.2482131244897\n",
       "\\item 19.3349840640291\n",
       "\\item 19.4244303104302\n",
       "\\item 19.0332138547041\n",
       "\\item 18.9786436582254\n",
       "\\item 18.8330450653182\n",
       "\\item 18.9611507120531\n",
       "\\item 19.0686299814602\n",
       "\\item 19.490932299329\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 24.2315135179292\n",
       "2. 19.2482131244897\n",
       "3. 19.3349840640291\n",
       "4. 19.4244303104302\n",
       "5. 19.0332138547041\n",
       "6. 18.9786436582254\n",
       "7. 18.8330450653182\n",
       "8. 18.9611507120531\n",
       "9. 19.0686299814602\n",
       "10. 19.490932299329\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] 24.23151 19.24821 19.33498 19.42443 19.03321 18.97864 18.83305 18.96115\n",
       " [9] 19.06863 19.49093"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv.error = rep(0, 10)\n",
    "for (i in 1:10){\n",
    "    glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)\n",
    "    cv.error[i] = cv.glm(Auto, glm.fit)$delta[1]\n",
    "}\n",
    "cv.error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is a sharp drop in the estimated test mean squared error between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials. This agrees with Figure 5.4 in ISLR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `cv.glm()` function to perform $k$-fold cross validation by supplying a value `K` to the function call. We'll use $k = 10$, a common choice for $k$, on the `Auto` data set and again use a `for` loop to iteratively compute the $k$-fold cross validation errors corresponding to polynomial fits of order $i = 1, \\dots, 10$. Note that since $k$-fold cross validation involves random sampling, we once again set a random seed to get reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>24.2720671232254</li>\n",
       "\t<li>19.2690928085129</li>\n",
       "\t<li>19.3480535605547</li>\n",
       "\t<li>19.2949648229745</li>\n",
       "\t<li>19.0319790002896</li>\n",
       "\t<li>18.8978121056401</li>\n",
       "\t<li>19.1206066690695</li>\n",
       "\t<li>19.1466631054789</li>\n",
       "\t<li>18.8701307442148</li>\n",
       "\t<li>20.9552042280394</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 24.2720671232254\n",
       "\\item 19.2690928085129\n",
       "\\item 19.3480535605547\n",
       "\\item 19.2949648229745\n",
       "\\item 19.0319790002896\n",
       "\\item 18.8978121056401\n",
       "\\item 19.1206066690695\n",
       "\\item 19.1466631054789\n",
       "\\item 18.8701307442148\n",
       "\\item 20.9552042280394\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 24.2720671232254\n",
       "2. 19.2690928085129\n",
       "3. 19.3480535605547\n",
       "4. 19.2949648229745\n",
       "5. 19.0319790002896\n",
       "6. 18.8978121056401\n",
       "7. 19.1206066690695\n",
       "8. 19.1466631054789\n",
       "9. 18.8701307442148\n",
       "10. 20.9552042280394\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666\n",
       " [9] 18.87013 20.95520"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(17)\n",
    "cv.error = rep(0, 10)\n",
    "for (i in 1:10){\n",
    "    glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)\n",
    "    cv.error[i] = cv.glm(Auto, glm.fit, K = 10)$delta[1]\n",
    "}\n",
    "cv.error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the computation time is much shorter than that of LOOCV; the computation was near instantaneous as opposed to taking about 30-45 seconds. While the formula\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{CV}_{(n)} = \\frac{1}{n} \\sum_{i = 1}^n \\left( \\frac{y_i - \\hat{y}_i}{1 - h_i} \\right)^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat{y}_i$ is the $i$th fitted value from the original least squares fit, and $h_i$ is the leverage value of the $i$th observation, could be used, in princlple, to greatly speed up the computation of LOOCV in the case of least squares or polynomial regression, `cv.glm()` does not make use of it.\n",
    "\n",
    "As with before, there is still little evidence that using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the use of the bootstrap, first we continue with the example shown in Section 5.2 of ISLR using the simulated `Portfolio` data set before moving on to an example involving estimating the accuracy of the linear regression model on the `Auto` data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Accuracy of a Statistic of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the strengths of the bootstrap approach is that it is very widely applicable and does not require complicated mathematical complications; in `R`, we only need to take two steps to perform a bootstrap analysis. The first is creating a function that computes the statistic of interest. Second, we use the `boot()` function from the `boot` library to perform the bootstrap by repeatedly sampling observations from the data with replacement.\n",
    "\n",
    "As already noted, we start by working the the `Portfolio` data set from the `ISLR` package, which described in Section 5.2 of ISLR. It consists of 100 simulated pairs of returns for the investments $X$ and $Y$. We are trying to choose the the fraction of our money $\\alpha$ to invest in $X$ (investing the remaining $1 - \\alpha$ in $Y$) that minimizes the variance of our investment. In other words, we want to minimize $\\text{Var}(\\alpha X + (1 - \\alpha)Y)$. It can be shown that the value of $\\alpha$ which minimizes the risk is given by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\alpha = \\frac{\\sigma_Y^2 - \\sigma_{XY}}{\\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{XY}},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\sigma_X^2 = \\text{Var}(X)$, $\\sigma_Y^2 = \\text{Var}(Y)$, and $\\sigma_{XY} = \\text{Cov}(X, Y)$. Since the `Portfolio` data set consists of simulated data, we know that the true values of the parameters were set to $\\sigma_X^2 = 1$, $\\sigma_Y^2 = 1.25$, and $\\sigma_{XY} = 0.5$.\n",
    "\n",
    "To illustrate our use of the bootstrap on this data, we first create a function, `alpha.fn()`, which takes two arguments:\n",
    "\n",
    "- `data`: the $(X, Y)$ data\n",
    "- `index`: a vector indicating which observations should be used to estimate $\\alpha$.\n",
    "\n",
    "The function then outputs the estimate for $\\alpha$ based on the selected observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha.fn = function(data, index){\n",
    "    X = data$X[index]\n",
    "    Y = data$Y[index]\n",
    "    return ((var(Y) - cov(X, Y))/(var(X) + var(Y) - 2*cov(X, Y)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns an estimate for $\\alpha$ by plugging in the estimates for $\\sigma_X^2 = \\text{Var}(X)$, $\\sigma_Y^2 = \\text{Var}(Y)$, and $\\sigma_{XY} = \\text{Cov}(X, Y)$ into the above formula for $\\alpha$. For example, we can use the function to estimate $\\alpha$ using all 100 observations in the `Portfolio` data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.57583207459283"
      ],
      "text/latex": [
       "0.57583207459283"
      ],
      "text/markdown": [
       "0.57583207459283"
      ],
      "text/plain": [
       "[1] 0.5758321"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha.fn(Portfolio, 1:100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the `sample()` function to randomly select 100 observations, with replacement, from the range 1 to 100. This is equivalent to constructing a new bootstrap data set and recomputing $\\hat{\\alpha}$ with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.736837501928544"
      ],
      "text/latex": [
       "0.736837501928544"
      ],
      "text/markdown": [
       "0.736837501928544"
      ],
      "text/plain": [
       "[1] 0.7368375"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(1)\n",
    "alpha.fn(Portfolio, sample(100, 100, replace = T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we could perform bootstrap analysis by performing this command many times, recording all of the corresponding estimates $\\hat{\\alpha}$, and computing the resulting standard deviation, it is much more convenient to use the `boot()` function to automate the process. Beow we produce $R = 1000$ bootstrap estimates for $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "ORDINARY NONPARAMETRIC BOOTSTRAP\n",
       "\n",
       "\n",
       "Call:\n",
       "boot(data = Portfolio, statistic = alpha.fn, R = 1000)\n",
       "\n",
       "\n",
       "Bootstrap Statistics :\n",
       "     original       bias    std. error\n",
       "t1* 0.5758321 -0.001695873  0.09366347"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boot(Portfolio, alpha.fn, R = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final output shows that when bootstrapping with the `Portfolio` data, $\\hat{\\alpha} = 0.5758$ and that the bootstrap estimate for $\\text{SE}(\\hat{\\alpha})$ is $0.0937$. This is fairly close to the true value $\\alpha = 0.6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Accuracy of a Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll apply the bootstrap to assess the variability of the coefficient estimates and predictions from a statistical learning method. We'll demonstrate that by using the bootstrap to assess the variability of the estimates of $\\beta_0$ and $\\beta_1$, the intercept and slope terms for the linear regression model that uses `horsepower` to predict `mpg` in the `Auto` data set. We'll then compare the bootstrap estimates with the ones obtained using the formulas for $\\text{SE}(\\hat{\\beta}_i)$ described in Section 3.1.2 of ISLR.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{SE}(\\hat{\\beta}_0)^2 = \\sigma^2 \\left[ \\frac{1}{n} + \\frac{\\bar{x}^2}{\\sum_{i = 1}^n (x_i - \\bar{x})^2} \\right], \\,\n",
    "    \\text{SE}(\\hat{\\beta}_1)^2 = \\frac{\\sigma^2}{\\sum_{i = 1}^n (x_i - \\bar{x})^2}\n",
    "\\end{equation}\n",
    "\n",
    "Recall that $\\sigma$ is estimated by using the residual standard error (RSE) $\\hat{\\sigma}^2$ for the model, which is computed using the formula\n",
    "\n",
    "\\begin{equation}\n",
    "    \\text{RSE} = \\hat{\\sigma} = \\sqrt{\\frac{1}{n - 2} \\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}.\n",
    "\\end{equation}\n",
    "\n",
    "To start, we create the function `boot.fn()` which takes in the `Auto` data set along with a set of indices for the observations, and returns the intercept and slope estimates for the linear regression model. Note that since the function is only one line long, we aren't required to include the curly braces `{` and `}` at the beginning and end of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot.fn = function(data, index)\n",
    "    return(coef(lm(mpg ~ horsepower, data = data, subset = index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll demonstrate applying this function to the full set of 392 observations in order to compute the estimates of $\\beta_0$ and $\\beta_1$ on the entire data set using the usual linear regression coefficient estimate formulas from Chapter 3 of ISLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>(Intercept)</dt>\n",
       "\t\t<dd>39.9358610211705</dd>\n",
       "\t<dt>horsepower</dt>\n",
       "\t\t<dd>-0.157844733353654</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[(Intercept)] 39.9358610211705\n",
       "\\item[horsepower] -0.157844733353654\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "(Intercept)\n",
       ":   39.9358610211705horsepower\n",
       ":   -0.157844733353654\n",
       "\n"
      ],
      "text/plain": [
       "(Intercept)  horsepower \n",
       " 39.9358610  -0.1578447 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boot.fn(Auto, 1:392)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we did with `alpha.fn()`, we'll use `boot.fn()` to create bootstrap estimates for the intercept and slope terms by randomly sampling from among the observations with replacement. Here are two examples before using the `boot()` function to actually peform the full bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>(Intercept)</dt>\n",
       "\t\t<dd>40.3404516830189</dd>\n",
       "\t<dt>horsepower</dt>\n",
       "\t\t<dd>-0.163486837689938</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[(Intercept)] 40.3404516830189\n",
       "\\item[horsepower] -0.163486837689938\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "(Intercept)\n",
       ":   40.3404516830189horsepower\n",
       ":   -0.163486837689938\n",
       "\n"
      ],
      "text/plain": [
       "(Intercept)  horsepower \n",
       " 40.3404517  -0.1634868 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(1)\n",
    "boot.fn(Auto, sample(392, 392, replace = T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>(Intercept)</dt>\n",
       "\t\t<dd>40.1186906449022</dd>\n",
       "\t<dt>horsepower</dt>\n",
       "\t\t<dd>-0.157706320543503</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[(Intercept)] 40.1186906449022\n",
       "\\item[horsepower] -0.157706320543503\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "(Intercept)\n",
       ":   40.1186906449022horsepower\n",
       ":   -0.157706320543503\n",
       "\n"
      ],
      "text/plain": [
       "(Intercept)  horsepower \n",
       " 40.1186906  -0.1577063 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boot.fn(Auto, sample(392, 392, replace = T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the `boot()` function to compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "ORDINARY NONPARAMETRIC BOOTSTRAP\n",
       "\n",
       "\n",
       "Call:\n",
       "boot(data = Auto, statistic = boot.fn, R = 1000)\n",
       "\n",
       "\n",
       "Bootstrap Statistics :\n",
       "      original        bias    std. error\n",
       "t1* 39.9358610  0.0544513229 0.841289790\n",
       "t2* -0.1578447 -0.0006170901 0.007343073"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boot(Auto, boot.fn, R = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got bootstrap estimates of 0.84 for $\\text{SE}(\\hat{\\beta}_0)$ and 0.0073 for $\\text{SE}(\\hat{\\beta}_1)$. Now we'll compare them with the estimates computed using the above formulas, which can be obtained using the `summary()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Estimate</th><th scope=col>Std. Error</th><th scope=col>t value</th><th scope=col>Pr(&gt;|t|)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>(Intercept)</th><td>39.9358610   </td><td>0.717498656  </td><td> 55.65984    </td><td>1.220362e-187</td></tr>\n",
       "\t<tr><th scope=row>horsepower</th><td>-0.1578447   </td><td>0.006445501  </td><td>-24.48914    </td><td> 7.031989e-81</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & Estimate & Std. Error & t value & Pr(>\\textbar{}t\\textbar{})\\\\\n",
       "\\hline\n",
       "\t(Intercept) & 39.9358610    & 0.717498656   &  55.65984     & 1.220362e-187\\\\\n",
       "\thorsepower & -0.1578447    & 0.006445501   & -24.48914     &  7.031989e-81\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Estimate | Std. Error | t value | Pr(>|t|) |\n",
       "|---|---|---|---|---|\n",
       "| (Intercept) | 39.9358610    | 0.717498656   |  55.65984     | 1.220362e-187 |\n",
       "| horsepower | -0.1578447    | 0.006445501   | -24.48914     |  7.031989e-81 |\n",
       "\n"
      ],
      "text/plain": [
       "            Estimate   Std. Error  t value   Pr(>|t|)     \n",
       "(Intercept) 39.9358610 0.717498656  55.65984 1.220362e-187\n",
       "horsepower  -0.1578447 0.006445501 -24.48914  7.031989e-81"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(lm(mpg ~ horsepower, data = Auto))$coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the formulas from Section 3.1.2, the standard error estimates are 0.717 for the intercept and 0.0064 for the slope, which are somewhat different from the bootstrap estimates. This is due to the assumptions underlying those formulas used for the estimation. \n",
    "\n",
    "- First, those formulas depend on the unknown parameter $\\sigma^2$, the noise variance, which we estimated using the residual sum of squares. While the formulas for the standard errors of the coefficients don't rely on the correctness of the model, our estimate for $\\sigma^2$ does. We saw previously that there is a non-linear relationship `mpg` and `horsepower`, which results in inflated residuals from a linear fit. In turn, this will affect our estimate of $\\sigma^2$.\n",
    "- In addition, the standard formulas make the (somewhat unrealistic) assumption that the $x_i$ are fixed, and that all of the variability comes from the variation in the errors $\\epsilon_i$.\n",
    "\n",
    "The bootstrap approach doesn't rely on any of these assumptions, so it is likely that the bootstrap estimates are more accurate estimates of the standard errors of $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ than those from the `summary()` function.\n",
    "\n",
    "Next, we use the bootstrap to compute standard error estimates for a quadratic model and compare them with the standard linear regression estimates. Since a quadratic model provides a good fit to the data, there is now a better correpondence between the bootstrap estimates and the standard estimates of $\\text{SE}(\\hat{\\beta}_i)$ for $i = 0, 1, 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "ORDINARY NONPARAMETRIC BOOTSTRAP\n",
       "\n",
       "\n",
       "Call:\n",
       "boot(data = Auto, statistic = boot.fn, R = 1000)\n",
       "\n",
       "\n",
       "Bootstrap Statistics :\n",
       "        original        bias     std. error\n",
       "t1* 56.900099702  3.511640e-02 2.0300222526\n",
       "t2* -0.466189630 -7.080834e-04 0.0324241984\n",
       "t3*  0.001230536  2.840324e-06 0.0001172164"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "boot.fn = function(data, index)\n",
    "    return(coef(lm(mpg ~ horsepower + I(horsepower^2), data = data, subset = index)))\n",
    "set.seed(1)\n",
    "boot(Auto, boot.fn, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Estimate</th><th scope=col>Std. Error</th><th scope=col>t value</th><th scope=col>Pr(&gt;|t|)</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>(Intercept)</th><td>56.900099702 </td><td>1.8004268063 </td><td> 31.60367    </td><td>1.740911e-109</td></tr>\n",
       "\t<tr><th scope=row>horsepower</th><td>-0.466189630 </td><td>0.0311246171 </td><td>-14.97816    </td><td> 2.289429e-40</td></tr>\n",
       "\t<tr><th scope=row>I(horsepower^2)</th><td> 0.001230536 </td><td>0.0001220759 </td><td> 10.08009    </td><td> 2.196340e-21</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       "  & Estimate & Std. Error & t value & Pr(>\\textbar{}t\\textbar{})\\\\\n",
       "\\hline\n",
       "\t(Intercept) & 56.900099702  & 1.8004268063  &  31.60367     & 1.740911e-109\\\\\n",
       "\thorsepower & -0.466189630  & 0.0311246171  & -14.97816     &  2.289429e-40\\\\\n",
       "\tI(horsepower\\textasciicircum{}2) &  0.001230536  & 0.0001220759  &  10.08009     &  2.196340e-21\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Estimate | Std. Error | t value | Pr(>|t|) |\n",
       "|---|---|---|---|---|\n",
       "| (Intercept) | 56.900099702  | 1.8004268063  |  31.60367     | 1.740911e-109 |\n",
       "| horsepower | -0.466189630  | 0.0311246171  | -14.97816     |  2.289429e-40 |\n",
       "| I(horsepower^2) |  0.001230536  | 0.0001220759  |  10.08009     |  2.196340e-21 |\n",
       "\n"
      ],
      "text/plain": [
       "                Estimate     Std. Error   t value   Pr(>|t|)     \n",
       "(Intercept)     56.900099702 1.8004268063  31.60367 1.740911e-109\n",
       "horsepower      -0.466189630 0.0311246171 -14.97816  2.289429e-40\n",
       "I(horsepower^2)  0.001230536 0.0001220759  10.08009  2.196340e-21"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(lm(mpg ~ horsepower + I(horsepower^2), data = Auto))$coef"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
